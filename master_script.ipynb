{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "import random\n",
    "import os\n",
    "\n",
    "from savingOutputs import *\n",
    "from loadData import *\n",
    "from masks import *\n",
    "from decoding import *\n",
    "from plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "classes = ['Up', 'Down', 'Right', 'Left']\n",
    "nb_runs = 12\n",
    "length = nb_runs * len(classes)\n",
    "subjects_ids = range(1, 24)  # TODO modify this to get all subjects (here we only have 1 for the example)\n",
    "n_subjects = len(subjects_ids)\n",
    "n_perms = 1000\n",
    "class_labels = [\"\"] * (2 * length)\n",
    "for i in range(length * 2):\n",
    "    class_labels[i] = classes[(i // nb_runs) % len(classes)]\n",
    "labels = dict()\n",
    "labels[\"vis\"] = np.array(class_labels[:length])\n",
    "labels[\"aud\"] = np.array(class_labels[length:])\n",
    "\n",
    "classical_tasks_regions = [([\"vis\"], [\"V5_R\", \"V5_L\"]),\n",
    "                           ([\"aud\"], [\"PT_R\", \"PT_L\"]),\n",
    "                           ([\"aud\"], [\"V5_R\", \"V5_L\"])]\n",
    "\n",
    "cross_modal_task_regions = [([\"vis\", \"aud\"], [\"V5_R\", \"V5_L\"])]\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "classifier = sklearn.svm.SVC(C = 1, kernel = 'linear', random_state=SEED)  # other parameters are default parameters\n",
    "pipeline = Pipeline([('std', scaler), ('svm', classifier)])\n",
    "decoder = Decoder(n_perm=n_perms, model=pipeline, n_classes=len(classes), n_splits=nb_runs, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = [dict() for _ in subjects_ids]\n",
    "p_values = [dict() for _ in subjects_ids]\n",
    "scores_perms = [dict() for _ in subjects_ids]\n",
    "maps_masked = [dict() for _ in subjects_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for i, subj_id in enumerate(subjects_ids):\n",
    "    t_maps, beta_maps = get_maps([subj_id])\n",
    "    masks = get_masks([subj_id], plot=False)\n",
    "    maps = apply_mask_to_maps(t_maps, masks)\n",
    "    maps_masked[i][\"vis\"] = get_part_of_maps(maps, 0, length)  # maps acquired for the vision experiment\n",
    "    maps_masked[i][\"aud\"] = get_part_of_maps(maps, length, 2 * length)  # maps acquired for the audition experiment\n",
    "    del t_maps ; del beta_maps ; del masks  # to relieve memory\n",
    "    print(\"Loading done for subject \"+str(subj_id)+\"/\"+str(n_subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Within-modality decoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for i, subj_id in enumerate(subjects_ids):\n",
    "    # within-modality decoding : training on a task and decoding on other samples from same task\n",
    "    for tasks, regions in classical_tasks_regions:\n",
    "        cv_sc, p_val, scores_perm = decoder.classify_tasks_regions(maps_masked[i], labels, tasks, regions)\n",
    "        cv_scores[i].update(cv_sc)\n",
    "        p_values[i].update(p_val)\n",
    "        scores_perms[i].update(scores_perm)\n",
    "    print(\"Within-modality decoding done for subject \"+str(subj_id)+\"/\"+str(n_subjects))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cross-modal decoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, subj_id in enumerate(subjects_ids):\n",
    "    # cross-modal decoding : training on a task and decoding on samples from another task\n",
    "    for tasks, regions in cross_modal_task_regions:\n",
    "        scores_cross_mod = decoder.cross_modal_decoding(maps_masked[i], labels, tasks, regions)\n",
    "        cv_scores[i].update(scores_cross_mod)\n",
    "\n",
    "    print(\"Cross-modal decoding done for subject \"+str(subj_id)+\"/\"+str(n_subjects))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_directory(\"out\")\n",
    "save_dicts(\"cv_scores.csv\", cv_scores, list(cv_scores[0].keys()), subjects_ids)\n",
    "save_dicts(\"p_values.csv\", p_values, list(p_values[0].keys()), subjects_ids)\n",
    "save_dicts_perms(\"scores_perms.csv\", scores_perms, subjects_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results (from files of saved results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt_directory = \"plots\"\n",
    "create_directory(plt_directory)\n",
    "plotter = Plotter(plt_directory, subjects_ids)\n",
    "plotter.plot_cv_pval(\"out/cv_scores.csv\", \"cv score\", plotter.cv_scores_dir, chance_level = True)\n",
    "plotter.plot_cv_pval(\"out/p_values.csv\", \"p-value\", plotter.p_values_dir)\n",
    "plotter.plot_perms_scores(\"out/scores_perms.csv\", n_perms)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualizing differences between audition and vision in the voxels of a ROI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "region = \"V5_R\"\n",
    "n_voxels = maps_masked[0][\"vis\"][0][region].shape[1]\n",
    "mean_aud = np.zeros(n_voxels)\n",
    "mean_vis = np.zeros(n_voxels)\n",
    "\n",
    "for i in range(n_subjects):\n",
    "    mean_vis += np.mean(maps_masked[i][\"vis\"][0][region],axis=0)/n_subjects\n",
    "    mean_aud += np.mean(maps_masked[i][\"aud\"][0][region],axis=0)/n_subjects\n",
    "\n",
    "plt.plot(range(n_voxels),mean_vis,label = \"vision\")\n",
    "plt.plot(range(n_voxels),mean_aud,label = \"audition\")\n",
    "plt.xlabel(\"voxel id\")\n",
    "plt.ylabel(\"intensity\")\n",
    "plt.title(\"Average voxel intensities for right V5\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/why_normalize_cross_modal.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}